{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21380\\596913931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcvxopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msolvers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspdiag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msoftsvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftsvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_small_eigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_random_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftsvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import solvers, matrix, spmatrix, spdiag, sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from softsvm import softsvm, fix_small_eigvals, error, get_random_sample\n",
    "import softsvm\n",
    "# unlimited np output\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "data = np.load('ex2q4_data.npz')\n",
    "trainX, testX = data['Xtrain'], data['Xtest']\n",
    "trainY, testY = data['Ytrain'], data['Ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(xi, xj, k):\n",
    "    \"\"\"\n",
    "    :param xi, xj: two vectors of the same dimension\n",
    "    :param k: the degree of the poly kernel\n",
    "    :return: (1 + dot_res)^k\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert xi.shape == xj.shape, \"xi and xj should be of the same shape\"\n",
    "    return np.power(np.dot(xi, xj) + 1.0, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_matrix(X, k):\n",
    "    print(X.shape)\n",
    "    G = np.dot(X, X.T)\n",
    "    # apply k to every item in G\n",
    "    G = np.vectorize(lambda x: np.power(x, k))(G)\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsvmpoly(l: float, k: float, trainX: np.array, trainy: np.array):\n",
    "    \"\"\"\n",
    "    :param l: the parameter lambda of the soft SVM algorithm\n",
    "    :param sigma: the bandwidth parameter sigma of the RBF kernel.\n",
    "    :param trainX: numpy array of size (m, d) containing the training sample\n",
    "    :param trainy: numpy array of size (m, 1) containing the labels of the training sample\n",
    "    :return: numpy array of size (m, 1) which describes the coefficients found by the algorithm\n",
    "    \"\"\"\n",
    "    m, d = trainX.shape\n",
    "    \n",
    "    G = get_gram_matrix(trainX, k)\n",
    "\n",
    "    H = np.pad(float(2 * l) * G, [(0, m), (0, m)])\n",
    "    H = fix_small_eigvals(H)\n",
    "\n",
    "    A = np.block([[np.zeros((m, m)), np.identity(m)],\n",
    "                  [G * trainy.reshape(-1, 1), np.identity(m)]])\n",
    "\n",
    "    u = np.hstack((np.full(m, float(0)), np.full(m, 1/m)))\n",
    "\n",
    "    v = np.hstack((np.zeros(m), np.ones(m)))\n",
    "  \n",
    "    z = solvers.qp(matrix(H), matrix(u), -matrix(A), -matrix(v))\n",
    "    alphas = np.array(z[\"x\"])[:m]\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(set_a: np.array, set_b: np.array):\n",
    "    return [(ai, bi) for ai in set_a for bi in set_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_sample(alphas: np.array, k: int, sample: np.array, trainX: np.array):\n",
    "    train_kernels = np.array([K(sample, xi, k) for xi in trainX])\n",
    "    return np.sign(np.dot(train_kernels, alphas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alphas: np.array, k: int, testX: np.array, trainX: np.array):\n",
    "    \"\"\"\n",
    "    :param alphas: numpy array of size (m, 1) containing the coefficients of the soft SVM algorithm\n",
    "    :param testX: numpy array of size (m, d) containing the test sample\n",
    "    :param k: int, the degree of the poly kernel\n",
    "    :return: numpy array of size (m, 1) containing the predicted labels of the test sample\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([predict_single_sample(alphas, k, sample, trainX) for sample in testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(folds: int):\n",
    "    X_chunks = np.array(np.split(trainX, folds))\n",
    "    Y_chunks = np.array(np.split(trainY, folds))\n",
    "    splitted = []\n",
    "    shp = X_chunks.shape\n",
    "    for i in range(folds):\n",
    "        test, test_labales = X_chunks[i], Y_chunks[i]\n",
    "        # current test set is the the entire train besides the current chunk used for training\n",
    "        train = np.concatenate(np.delete(X_chunks, i, axis=0))\n",
    "        train_labales = np.concatenate(np.delete(Y_chunks, i, axis=0))\n",
    "        splitted.append({\"train\": train,\n",
    "                        \"train_labales\": train_labales,\n",
    "                        \"test\": test,\n",
    "                        \"test_labales\": test_labales})\n",
    "    return np.array(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_cross_validation(lambdas: np.array, ks, folds: int):\n",
    "    \"\"\"\n",
    "    find pair (lambda, k) with the lowest validation error, and get classifier based on that pair\n",
    "    :param lambdas: the lambda parameters to use\n",
    "    :param ks: the k parameters to use in the kernel function\n",
    "    :param folds: number of chunks we split the data to\n",
    "    :return: prediction of test set of classifier trained on the entire train set using best lambda and k found\n",
    "    \"\"\"\n",
    "    # poly softSVM\n",
    "    errors = {(l, k): 0 for l, k in cartesian_product(lambdas, ks)}\n",
    "    a = split_data(folds)\n",
    "    for fold in split_data(folds):\n",
    "        for l, k in cartesian_product(lambdas, ks):\n",
    "            print(l,k)\n",
    "            alphas = softsvmpoly(l, k, fold[\"train\"], fold[\"train_labales\"])\n",
    "            predicted = predict(alphas, k, fold[\"test\"], fold[\"train\"])\n",
    "            # continuesly calculate the avg error\n",
    "            errors[(l, k)] += error(fold[\"test_labales\"], predicted) / folds\n",
    "    \n",
    "    # get the pair with lowest avg error\n",
    "    best_lambda, best_k = min(errors.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"best_lambda={best_lambda}, best_k={best_k}\")\n",
    "    alphas = softsvmpoly(best_lambda, best_k, trainX, trainY)\n",
    "    return predict(alphas, best_k, testX, trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsvm_cross_validation(lambdas, folds):\n",
    "    errors = {l: 0 for l in lambdas}\n",
    "    for fold in split_data(folds):\n",
    "        for l in lambdas:\n",
    "            w = softsvm.softsvm(l, fold[\"train\"], fold[\"train_labales\"])\n",
    "            errors[l] += error(fold[\"test_labales\"], softsvm.predict(w, fold[\"test\"])) / folds\n",
    "    \n",
    "    # get the l with lowest avg error\n",
    "    best_lambda = min(errors.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"best_lambda={best_lambda}\")\n",
    "    w = softsvm.softsvm(best_lambda, trainX, trainY)\n",
    "    return softsvm.predict(w, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_error():\n",
    "    lambdas = np.array([1.0, 10.0, 100.0])\n",
    "    ks = np.array([2.0, 5.0, 8.0])\n",
    "    k_folds = 5\n",
    "    # polynomial kernel\n",
    "    poly_predicions = poly_cross_validation(lambdas, ks, folds=k_folds)\n",
    "    poly_error = error(testY, poly_predicions)\n",
    "\n",
    "    # linear softsvm\n",
    "    soft_svm_predictions = softsvm_cross_validation(lambdas, folds=k_folds)\n",
    "    soft_svm_error =  error(testY, soft_svm_predictions)\n",
    "\n",
    "    print(f\"soft_svm_error={soft_svm_error}\")\n",
    "    print(f\"poly_error={poly_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\uni\\Machine Learning intro\\Introduction-To-Machine-Learning\\MLenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Get the path to the Python interpreter\n",
    "python_path = sys.executable\n",
    "\n",
    "# Print the path\n",
    "print(python_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ed5fa07ca9cd55beeef47f224bbbd5867169d4ebe490c55adacd701e94c428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
