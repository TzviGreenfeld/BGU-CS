{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# set the path so it can import the kmeans.py file\n",
    "import sys\n",
    "sys.path.append('ex3')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "data = np.load(\"mnist_all.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_dist(c1, c2):\n",
    "    return np.array([np.linalg.norm(x1 - x2) for x1 in c1 for x2 in c2]).min()\n",
    "\n",
    "\n",
    "def singlelinkage(X, k):\n",
    "    \"\"\"\n",
    "    :param X: numpy array of size (m, d) containing the test samples\n",
    "    :param k: the number of clusters\n",
    "    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of the cluster in which x_i has been assigned.\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    clusters = [np.array([i]) for i in range(m)]\n",
    "    while len(clusters) > k:\n",
    "        distances = np.array([[cluster_dist(clusters[i], clusters[j])\n",
    "                               for j in range(len(clusters))] for i in range(len(clusters))])\n",
    "        np.fill_diagonal(distances, np.inf)\n",
    "        min_i, min_j = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "        clusters[min_i] = np.concatenate((clusters[min_i], clusters[min_j]))\n",
    "        clusters.pop(min_j)\n",
    "    res = np.zeros((m, 1))\n",
    "    for i, c in enumerate(clusters):\n",
    "        for x_index in c:\n",
    "            res[x_index] = i\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    \"\"\"\n",
    "    Initialize centroids by randomly selecting k data points from X\n",
    "    \"\"\"\n",
    "    return X[np.random.choice(range(X.shape[0]), size=k)]\n",
    "\n",
    "\n",
    "def get_distances(X, k, centroids):\n",
    "    \"\"\"\n",
    "    Get the distance of each data point from each centroid\n",
    "    \"\"\"\n",
    "    return np.array([np.linalg.norm(X - centroids[i], axis=1) for i in range(k)]).T\n",
    "\n",
    "\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"\n",
    "    Assign each data point to the cluster with the nearest centroid\n",
    "    \"\"\"\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "def compute_new_centroids(X, k, clusters):\n",
    "    \"\"\"\n",
    "    Compute new centroids for each cluster by taking the mean of all data points in the cluster\n",
    "    \"\"\"\n",
    "    return np.array([X[clusters == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "\n",
    "def kmeans(X, k, t):\n",
    "    \"\"\"\n",
    "    :param X: numpy array of size (m, d) containing the test samples\n",
    "    :param k: the number of clusters\n",
    "    :param t: the number of iterations to run\n",
    "    :return: a column vector of length m, where C(i) ∈ {1, . . . , k} is the identity of the cluster in which x_i has been assigned.\n",
    "    \"\"\"\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    clusters = None\n",
    "    for _ in range(t):\n",
    "        distances = get_distances(X, k, centroids)\n",
    "        clusters = assign_clusters(distances)\n",
    "        centroids = compute_new_centroids(X, k, clusters)\n",
    "\n",
    "    return clusters.reshape((clusters.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kmeans_unlabeld(k):\n",
    "    X = np.concatenate([data[f'train{i}'] for i in range(10)])\n",
    "    Y = np.concatenate(\n",
    "        [np.full((data[f'train{i}'].shape[0], 1), i) for i in range(10)])\n",
    "    \n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    samples = np.array(X[indices][:1000])\n",
    "    labels = Y[indices][:1000]\n",
    "\n",
    "    kmeans_res = kmeans(samples, k, 20)\n",
    "\n",
    "    clusters = np.empty((k,), dtype=object)\n",
    "    for i in range(k):\n",
    "        indices = np.where(kmeans_res == i)[0]\n",
    "        clusters[i] = indices\n",
    "    \n",
    "    # clusters size \n",
    "    clsuters_size = np.array([len(cluster) for cluster in clusters])\n",
    "\n",
    "    #  most common \n",
    "    real_labels_per_cluster = [[labels[i] for i in cluster] for cluster in clusters]\n",
    "    most_common_label_in_cluster = []\n",
    "    for real_labels in real_labels_per_cluster:\n",
    "        if len(real_labels) > 0:\n",
    "            most_common_label_in_cluster.append(max(real_labels, key=real_labels.count))\n",
    "        else:\n",
    "            most_common_label_in_cluster.append(-1)\n",
    "\n",
    "    precent_of_most_common_label_in_cluster = [real_labels.count(\n",
    "        most_common_label_in_cluster[i]) / clsuters_size[i] for i, real_labels in enumerate(real_labels_per_cluster)]\n",
    "\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        if clsuters_size[i] == 0:\n",
    "            errors.append(0)\n",
    "        else:\n",
    "            errors.append(np.mean((np.array([most_common_label_in_cluster[i] for _ in range(\n",
    "                clsuters_size[i])]).reshape(-1, 1)) != real_labels_per_cluster[i]))\n",
    "                \n",
    "    return [clsuters_size, most_common_label_in_cluster, precent_of_most_common_label_in_cluster, errors]\n",
    "\n",
    "def analyze_single_linkage_unlabeld(k):\n",
    "    X = np.concatenate([data[f'train{i}'] for i in range(10)])\n",
    "    Y = np.concatenate([np.full((data[f'train{i}'].shape[0], 1), i) for i in range(10)])\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    samples = np.array(X[indices][:300])\n",
    "    labels = Y[indices][:300]\n",
    "\n",
    "    single_linkage_res = singlelinkage(samples, k)\n",
    "    clusters = np.empty((k,), dtype=object)\n",
    "    for i in range(k):\n",
    "        indices = np.where(single_linkage_res == i)[0]\n",
    "        clusters[i] = indices\n",
    "\n",
    "    clsuters_size = np.array([len(cluster) for cluster in clusters])\n",
    "\n",
    "    real_labels_per_cluster = [[labels[i]\n",
    "                                for i in cluster] for cluster in clusters]\n",
    "    most_common_label_in_cluster = []\n",
    "    for real_labels in real_labels_per_cluster:\n",
    "        if len(real_labels) > 0:\n",
    "            most_common_label_in_cluster.append(\n",
    "                max(real_labels, key=real_labels.count))\n",
    "        else:\n",
    "            most_common_label_in_cluster.append(-1)\n",
    "    \n",
    "    precent_of_most_common_label_in_cluster = [real_labels.count(most_common_label_in_cluster[i]) / clsuters_size[i] for i,real_labels in enumerate(real_labels_per_cluster)]\n",
    "    # format the precent to 4 digits after the dot\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        if clsuters_size[i] == 0:\n",
    "            errors.append(0)\n",
    "        else:\n",
    "            errors.append(np.mean((np.array([most_common_label_in_cluster[i] for _ in range(\n",
    "                clsuters_size[i])]).reshape(-1, 1)) != real_labels_per_cluster[i]))\n",
    "    \n",
    "    table = [clsuters_size, most_common_label_in_cluster, precent_of_most_common_label_in_cluster, errors]\n",
    "    return table  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(data, cols, rows, filename=None):\n",
    "    local_data = data.copy()\n",
    "    #  multply data[2] by 100 and only 2 digits after the dot\n",
    "    local_data[2] = [f\"{x*100:.2f}%\" for x in local_data[2]]\n",
    "    local_data[3] = [f\"{x:.2f}\" for x in local_data[3]]\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    the_table = ax.table(cellText=local_data, colLabels=cols, rowLabels=rows, cellLoc = 'center', loc='center')\n",
    "    plt.show()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\"Cluster Size\", \"Most Common Label\", \"Precent of Most Common Label\", \"Error\"]\n",
    "cols = [f\"Cluster {i}\" for i in range(10)]\n",
    "# k = 10\n",
    "table = analyze_kmeans_unlabeld(10)\n",
    "make_table(table, cols, rows, \"1c_kmeans_table.png\")\n",
    "table = analyze_single_linkage_unlabeld(10)\n",
    "make_table(table, cols, rows, \"1d_single_linkage_table.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\"Cluster Size\", \"Most Common Label\", \"Precent of Most Common Label\", \"Error\"]\n",
    "cols = [f\"Cluster {i}\" for i in range(6)]\n",
    "# k = 6\n",
    "table = analyze_kmeans_unlabeld(6)\n",
    "make_table(table, cols, rows, \"1e_kmeans_table_k_6.png\")\n",
    "# table = analyze_single_linkage_unlabeld(6)\n",
    "# make_table(table, cols, rows, \"1e_single_linkage_table_k_6.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ed5fa07ca9cd55beeef47f224bbbd5867169d4ebe490c55adacd701e94c428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
